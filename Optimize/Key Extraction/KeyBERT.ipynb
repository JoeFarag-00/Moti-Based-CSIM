{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2330,  4886,  1521,  1055, 23222,  7861,  8270,  4667,  1998,\n",
       "         14193,  4275,  4329,  4697, 17953,  2361,  1998,  4613,  5038,  1010,\n",
       "         20226, 10640,  1998,  8122,  1012,  2023,  9927, 15102,  2330,  4886,\n",
       "          7861,  8270,  4667,  2015,  1521,  4022,  2005,  3935, 17953,  2361,\n",
       "          8518,  1010,  2096,  1996,  2279,  7679,  2006,  7204, 14193,  4275,\n",
       "          1012,  2057,  3972,  3726,  2046,  2773,  7861,  8270,  4667,  2015,\n",
       "          1521, 24078,  1010, 12637,  1010,  1998,  2330,  4886,  1521,  1055,\n",
       "          6020,  2836,  1012,  7523,  5097,  2066,  3793, 14402,  1010, 21641,\n",
       "          3945,  1010,  1998,  9324,  2075,  1010,  2004,  2057,  4895,  3726,\n",
       "          4014,  2330,  4886,  7861,  8270,  4667,  2015,  1521, 10938,  8082,\n",
       "          2373,  1999, 17953,  2361,  1012,   102]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"OpenAI’s groundbreaking embedding and transcription models revolutionize NLP and speech recognition, enhancing accuracy and efficiency. This blog explores OpenAI Embeddings’ potential for advanced NLP tasks, while the next focuses on Whisper transcription models. We delve into word embeddings’ basics, advantages, and OpenAI’s superior performance. Discover applications like text similarity, semantic search, and clustering, as we unveil OpenAI Embeddings’ transformative power in NLP.\"\n",
    "tokens = tokenizer.tokenize(paragraph)\n",
    "input_ids = tokenizer.encode(paragraph, return_tensors='pt', max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "sentence_embeddings = torch.mean(embeddings, dim=1)\n",
    "# print(sentence_embeddings)\n",
    "similarity_matrix = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 25, 21, 20, 19, 18, 17, 16, 15, 14, 12, 11, 10,  9,  8,  7,  6,\n",
       "        5,  4,  2,  1, 46, 23, 35, 34, 45, 43, 42, 40, 39, 38, 37, 36,  0,\n",
       "       33, 32, 31, 30, 29, 28, 27, 41, 44, 22, 13, 24, 26,  3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([paragraph])\n",
    "sentence_lengths = X.toarray()[0]\n",
    "\n",
    "importance_scores = similarity_matrix.sum(axis=1) / sentence_lengths\n",
    "\n",
    "sorted_indices = importance_scores.argsort()[::-1]\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 key points: ['transcription', 'blog', 'and']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_key_points = [tokens[i] for i in sorted_indices[:3]]  \n",
    "print(\"Top 3 key points:\", top_key_points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
